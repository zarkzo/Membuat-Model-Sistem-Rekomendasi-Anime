# -*- coding: utf-8 -*-
"""customer_retail_recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Q3o2FaxYCL7ENPLi3yGP-47syCxJ1OT

# Proyek Akhir : Membuat Model Sistem Rekomendasi Anime
- **Nama:** Putra Faaris Prayoga
- **Email:** putrafaariz47@gmail.com
- **ID Dicoding:** putra_faaris

## Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag, word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow import keras
from scipy.sparse import hstack

# NLTK resource
nltk.download('punkt')
nltk.download('punkt_tab')  # untuk berjaga-jaga, walau ini tidak umum
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')

"""## Loading data"""

df = pd.read_csv("animes.csv")

df.head()

df.describe()

"""# EDA

"""

# Menghitung jumlah kemunculan genre
genres = df['genre'].str.split(', ')
all_genres = genres.explode()
genre_counts = all_genres.value_counts()

# Plot 5 genre terbanyak
top_genres = genre_counts.head(5)

plt.figure(figsize=(8, 5))
sns.barplot(x=top_genres.values, y=top_genres.index, palette='viridis')
plt.title('5 Genre Anime Terbanyak')
plt.xlabel('Jumlah')
plt.ylabel('Genre')
plt.tight_layout()
plt.show()

# Menghitung jumlah genre unik
total_unique_genres = all_genres.nunique()
print("Jumlah genre unik:", total_unique_genres)

unique_genres = all_genres.unique()
print("Daftar genre unik:")
print(sorted(unique_genres))

# distribusi skor

plt.figure(figsize=(8, 4))
sns.histplot(df['score'], bins=10, kde=True)
plt.title("Distribusi Skor Anime")
plt.xlabel("Score")
plt.ylabel("Jumlah")
plt.tight_layout()
plt.show()

"""## Preprocess

"""

# CLEANING DATA

df.drop_duplicates(inplace=True)
df.dropna(subset=['title', 'synopsis', 'genre', 'score', 'ranked'], inplace=True)
df = df[df['score'] > 0]
df = df[df['episodes'] > 0]
df.reset_index(drop=True, inplace=True)

#  TEXT PREPROCESSING (synopsis kata penting saja)

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = word_tokenize(text)
    keywords = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(keywords)

print("Preprocessing synopsis...")
df['clean_synopsis'] = df['synopsis'].apply(preprocess_text)

# FEATURE ENGINEERING

vectorizer = TfidfVectorizer(max_features=3000)
tfidf_matrix = vectorizer.fit_transform(df['clean_synopsis'])

genre_matrix = df['genre'].str.get_dummies(sep=', ')
scaled_features = MinMaxScaler().fit_transform(df[['score', 'ranked']])
final_features = hstack([tfidf_matrix, genre_matrix.values, scaled_features])

# CONTENT-BASED RECOMMENDER FUNCTION

cosine_sim = cosine_similarity(final_features)
indices = pd.Series(df.index, index=df['title']).drop_duplicates()

def get_recommendations(title, cosine_sim=cosine_sim):
    if title not in indices:
        return f"Anime '{title}' tidak ditemukan."
    idx = indices[title]
    if isinstance(idx, pd.Series):
        idx = idx.iloc[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]
    anime_indices = [i[0] for i in sim_scores]
    return df[['title', 'genre', 'score', 'ranked']].iloc[anime_indices]

"""## Model"""

# OPTIONAL: RECOMMENDER MODEL (RecommenderNet)

df['anime_id'] = LabelEncoder().fit_transform(df['title'])
x = df[['anime_id']].values
y = df['score'].values

scaler = MinMaxScaler()
y_scaled = scaler.fit_transform(y.reshape(-1, 1))

split = int(0.8 * len(df))
x_train, x_test = x[:split], x[split:]
y_train, y_test = y_scaled[:split], y_scaled[split:]

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_items, embed_size):
        super(RecommenderNet, self).__init__()
        self.item_embedding = layers.Embedding(num_items, embed_size,
                                               embeddings_initializer='he_normal',
                                               embeddings_regularizer=keras.regularizers.l2(1e-4))
        self.item_bias = layers.Embedding(num_items, 1)

    def call(self, inputs):
        item_vector = self.item_embedding(inputs[:, 0])
        item_bias = self.item_bias(inputs[:, 0])
        dot_product = tf.reduce_sum(item_vector * item_vector, axis=1, keepdims=True)
        x = tf.nn.sigmoid(dot_product + item_bias)
        return x

num_items = df['anime_id'].nunique()
model = RecommenderNet(num_items=num_items, embed_size=64)
model.compile(
    loss='binary_crossentropy',
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=['mae']
)

print("\nTraining RecommenderNet (Optional)...")
history = model.fit(x_train, y_train, batch_size=16, epochs=50, verbose=1)

"""## TEST dan Evaluasi"""

# EVALUASI MODEL

eval_result = model.evaluate(x_test, y_test)
print("Evaluation result (binary_crossentropy, MAE):", eval_result)

# TEST RECOMMENDATION

test_titles = [
    "Fullmetal Alchemist: Brotherhood",
    "Shigatsu wa Kimi no Uso",
    "Made in Abyss",
    "Haikyuu!! Second Season",
    "Steins;Gate"
]

for title in test_titles:
    print(f"\nTop 10 rekomendasi untuk '{title}':")
    print(get_recommendations(title))